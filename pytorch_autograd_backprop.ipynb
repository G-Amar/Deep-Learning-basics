{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQ0P23fPHBcFRvHzr0GmjR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"nv5fkhYPf0aK","executionInfo":{"status":"ok","timestamp":1694138138871,"user_tz":240,"elapsed":3,"user":{"displayName":"Amar Gupta","userId":"11346193758692241046"}}},"outputs":[],"source":["#pytorch `torch.autograd` lets us automatically compute the gradient so we can use it for back-propagation\n"]},{"cell_type":"code","source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True) #this allows ut to compute gradients\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"],"metadata":{"id":"Rtq_Q3utgGsE","executionInfo":{"status":"ok","timestamp":1694138170540,"user_tz":240,"elapsed":5779,"user":{"displayName":"Amar Gupta","userId":"11346193758692241046"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(f\"Gradient function for z = {z.grad_fn}\")\n","print(f\"Gradient function for loss = {loss.grad_fn}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ly8dH0hzgORc","executionInfo":{"status":"ok","timestamp":1694138293868,"user_tz":240,"elapsed":113,"user":{"displayName":"Amar Gupta","userId":"11346193758692241046"}},"outputId":"56af3d6b-a895-4a9a-8d9e-85e905391410"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient function for z = <AddBackward0 object at 0x7df4513aed10>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7df451408880>\n"]}]},{"cell_type":"code","source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7DIMGdj5gtxN","executionInfo":{"status":"ok","timestamp":1694138312481,"user_tz":240,"elapsed":135,"user":{"displayName":"Amar Gupta","userId":"11346193758692241046"}},"outputId":"6c574e81-5edf-46c9-ced7-bd374f98b561"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1282, 0.0171, 0.0993],\n","        [0.1282, 0.0171, 0.0993],\n","        [0.1282, 0.0171, 0.0993],\n","        [0.1282, 0.0171, 0.0993],\n","        [0.1282, 0.0171, 0.0993]])\n","tensor([0.1282, 0.0171, 0.0993])\n"]}]},{"cell_type":"code","source":["z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","#this allows us to prevent gradient tracking in the block, more efficient\n","# useful whe we no longer want back-propagation for a bit\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","# can also use detatch() method on a tensor to do the same thing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-MOjmstgyRv","executionInfo":{"status":"ok","timestamp":1694138481472,"user_tz":240,"elapsed":141,"user":{"displayName":"Amar Gupta","userId":"11346193758692241046"}},"outputId":"7105a15d-2178-4263-9298-17a5a64b42ff"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"code","source":["#autograd keeps data (tensor) and executed operations (on each tensor) in a DAG,\n","# using this DAG, leaves are input tensors, and roots are output tensors,\n","# tracing from root to leaves, can compute gradients with chain rule\n","\n","#in forward pass, autograd runs the operation and maintain the gradient funtion int the DAG\n","#in backward(), autograd computes gradients for each input, accumulated them for each respective tensor\n","#uses the chain rule, propagating weights all the way to the leaf tensor\n","\n","#DAGs in pytorch are dynamic and recreated after each .backward() call\n"],"metadata":{"id":"RKUt8JVzhblZ"},"execution_count":null,"outputs":[]}]}